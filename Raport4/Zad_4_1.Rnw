\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dodatkowe pakiety LaTeX'a
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ustawienia globalne
<<ustawienia_globalne, echo=FALSE, warning=FALSE>>=
library(knitr)
opts_chunk$set(
  fig.path='figure/',
  fig.align='center',
  fig.pos='H',
  fig.width=5,
  fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% strona tytulowa


\title{Raport nr 3}
\author{Emilia Kowal [249716], Jakub Dworzański [249703]}
\maketitle
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Zad. 2
<<ustalenie.ziarna, echo=FALSE>>=
set.seed(12062020)
@

<<biblioteki, echo=FALSE, eval=TRUE, warning=FALSE, results=FALSE, message=FALSE>>=
library(adabag)
library(e1071)
library(ipred)
library(mlbench)
library(randomForest)
library(rpart)
library(rpart.plot)
@

\section{Zaawansowane metody klasyfikacji}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krótki opis zagadnienia}

W tym ćwiczeniu, będziemy kontynuowali analizę zbioru danych o odłamkach szkła.
<<przykladowe_dane, echo=FALSE>>=
data(Glass)
kable(head(Glass), caption="Przykładowe dane", format="latex")
@
<<przykladowe_dane_pokaz ,eval=FALSE>>=
data(Glass)
head(Glass)
@

% https://archive.ics.uci.edu/ml/datasets/glass+identification

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opis eksperymentów/analiz}

Tym razem skorzystamy z bardziej zaawansownych metod.
Rozpoczniemy od zastosowania rodzin klasyfikatorów.
Skupimy się na algorytmach bagging, boosting oraz random forest.

Następnie, 
Będą to metoda k-najbliższych sąsiadów (k-NN, \emph{k-Nearest Neighbours}),
naiwny klasyfikator bayesowski (NBC, \emph{Naive Bayesian Classifier}) oraz
drzewo klasyfikacyjne (\emph{classification tree}).

Ponadto, postaramy się dobrać odpowiednie parametry dla tych modeli.
Spróbujemy również dokonać pewnych przekształceń danych, czy też
wybrać pewne podzbiory cech, aby sprawdzić, czy uda nam się w ten
sposób poprawić jakość klasyfikatora.

Po zbadaniu podstawowych metod, spróbujemy również porównać
je z metodami bardziej zaawansowanymi i wyrafinowanymi.

Aby ocenić jakość predykcji, będziemy korzystać
z 5-, 10-krotnego sprawdzianu krzyżowego
oraz sprawdzianu krzyżowego typu leave-one-out
(\emph{5-, 10-, leave-one-out Cross Validation}).
Ponadto, wykorzystamy metodę \emph{bootstrap} oraz \emph{632+}.

Zastosujemy do tego następujące funkcje pomocnicze,
odpowiadające za uczenie modelu oraz estymację błędów.

<<funkcje.pomocnicze>>=
mypredict <- function(model, newdata) predict(model, newdata, type="class")
adaboost.predict <- function(model, newdata){
  return(as.factor(predict(model, newdata)$class))
}
nasz.errorest <- function(formula, data, model, predict=mypredict, ...){
  cv.5 <- errorest(
    formula=formula, data=data, model=model, predict=predict,
    estimator="cv", est.para=control.errorest(k = 5), ...
  )$error
  bootstrap <- errorest(
    formula=formula, data=data, model=model,
    predict=predict, estimator="boot", ...
  )$error
  err632plus <- errorest(
    formula=formula, data=data, model=model,
    predict=predict, estimator="632plus", ...
  )$error
  return(data.frame(
    paste0(
      round(100*c(cv.5, bootstrap, err632plus), 2),
      "%"
    ),
    row.names=c(
      "CV5",
      "bootstrap (15 powtórzeń)",
      "632+ (15 powtórzeń)"
    )
  ))
}
wyniki <- data.frame(row.names=c(
      "CV5",
      "bootstrap (15 powtórzeń)",
      "632+ (15 powtórzeń)"
    ))
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wyniki}

\subsubsection{Rodziny klasyfikatorów}

Aby mieć punkt odniesienia podczas oceny wyników,
skorzystamy z drzewa decyzyjnego, które będzie
wykorzystywane jako klasyfikator bazowy w tym ćwiczeniu.

Przypomnijmy, że w pierwszej części analizy,
najlepszy rezultat osiągneliśmy poprzez tworzenie
drzewa na podstawie 6 najważniejszych zmiennych
oraz przycinanie go po konstrukcji za pomocą
metody jednego odchylenia.
<<waznosc.zmiennych.drzewo.pokaz, eval=FALSE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
drzewo$variable.importance
@
<<waznosc.zmiennych.drzewo, echo=FALSE, cache=TRUE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
kable(drzewo$variable.importance, col.names="Ważność zmiennej", format="latex",
      caption="Ważność zmiennych na podstawie konstrukcji drzewa klasyfikacyjnego")
@

<<poprzednie.wyniki.pokaz, eval=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
)
@
<<poprzednie.wyniki, echo=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
kable(
  nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
  ),
  col.names=c("Błąd klasyfikacji"),
  caption="Najlepsze wyniki dla drzewa klasyfikacyjnego, otrzymane podczas pierwszej części analizy."
)
@

<<drzewo.wykres.pokaz, eval=FALSE>>=
drzewo <- nasze.drzewo(Type ~ RI + Al + Ca + Mg + Na + Ba, Glass)
rpart.plot(drzewo)
@
<<drzewo.wykres, echo=FALSE, fig.cap="Przykładowe drzewo klasyfikacyjne", warning=FALSE>>=
drzewo <- nasze.drzewo(Type ~ RI + Al + Ca + Mg + Na + Ba, Glass)
rpart.plot(drzewo, extra=0, yes.text="tak", no.text="nie")
@

Na rys. \ref{fig:drzewo.wykres} możemy zobaczyć
przykładowe drzewo decyzyjne, stworzone tą metodą
na podstawie całego zbioru danych.

<<eval=FALSE>>=
wyniki["bagging"] <- nasz.errorest(
  Type ~ ., Glass, ipred::bagging
)
wyniki["boosting"] <- nasz.errorest(
  Type~., Glass, adabag::boosting, predict=adaboost.predict
)
@



\subsubsection{Bagging}
Rozpoczniemy od algorytmu bagging.







\subsubsection{Metoda wektorów nośnych (SVM)}

Analizę algorytmu SVM rozpoczniemy
od sprawdzenia błędu klasyfikacji
dla domyślnych parametrów,
korzystając z różnych funkcji jądrowych.

<<svm.wstep.pokaz, eval=FALSE>>=
wyniki["linear SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="linear")
wyniki["polynomial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="polynomial")
wyniki["radial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="radial")

wyniki
@

<<svm.wstep, echo=FALSE, eval=TRUE, cache=TRUE>>=
wyniki["linear SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="linear")
wyniki["polynomial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="polynomial")
wyniki["radial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="radial")
wyniki["sigmoid SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="sigmoid")
kable(
  wyniki[c("linear SVM", "polynomial SVM", "radial SVM", "sigmoid SVM")],
  col.names=c("liniowe", "wielomianowe", "radialne", "sigmoidalne"),
  caption="Porównanie SVM z wykorzystaniem domyślnych parametrów i różnych funkcji jądrowych."
)
@

W tabeli \ref{tab:svm.wstep} widzimy,
że dla domyślnych parametrów,
najlepiej radzą sobie
SVM z liniową funkcją jądrową
oraz SVM z radialną funkcją jądrową.
W tej samej tabeli,
widzimy również,
że wybór funkcji jądrowej
w sposób znaczny
wpływa na dokładność klasyfikatora.
Możemy jednak przypuszczać, że ma
to również związek z większą wrażliwością
na dobór parametrów dla SVM
z wielomianową lub sigmoidalną funkcją jądrową.


Postaramy się teraz sprawdzić,
jaki wpływ na klasyfikację,
przy pomocy SVM z jądrem liniowym,
ma parametr kosztu \emph{C}.

<<svm.linear.tuning.plot.pokaz, eval=FALSE>>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="linear", cost=2^(-4:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)
ggplot(svm.tuning$performances, aes(x=cost, y=error)) + geom_point()
svm.tuning$best.parameters
svm.tuning$best.performance
@
<<svm.linear.tuning.plot, echo=FALSE, cache=TRUE, fig.cap="Wykres błędu klasyfikacji w zależności od parametru kosztu dla SVM z liniową funkcją jądrową.">>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="linear", cost=2^(-4:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)

ggplot(svm.tuning$performances,
       aes(x=cost, y=error)
       ) +
  geom_point() +
  labs(x="Parametr kosztu", y="Błąd klasyfikacji")
kable(
  data.frame(
    svm.tuning$best.parameters,
    paste0(round(100*svm.tuning$best.performance, 2), "%")),
  col.names=c("Parametr kosztu", "Błąd klasyfikacji"),
  row.names=FALSE,
  caption="Parametry klasyfikacji dla SVM z jądrem liniowym, minimalizujące błąd klasyfikacji."
)
@

Na rysunku \ref{fig:svm.linear.tuning.plot}
widzimy, że
błąd klasyfikacji zmienia się
w zależności od parametru kosztu.
Spośród zbadanych przez nas parametrów,
estymowany błąd klasyfikacji
osiąga minimum dla $C=???$.
Otrzymujemy wtedy wynik ?????\%
(tab. \ref{tab:svm.linear.tuning.plot}).

Widzimy, że nie doszło do znacznego
zwiększenia dokładności klasyfikacji
względem domyślnych parametrów.
Dlatego też, spróbujemy teraz
"dostroić" parametry dla SVM
z radialną funkcją jądrową,
ponieważ może być on
bardziej wrażliwy na
dobór parametrów.

<<svm.radial.tuning.pokaz, eval=FALSE>>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="radial",
  cost=1.5^(-5:10), gamma=1.5^(-10:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)
ggplot(svm.tuning$performances, aes(x=cost, y=gamma, fill=error)) + 
  geom_tile() + scale_y_log10() + scale_x_log10()
svm.tuning$best.parameters
svm.tuning$best.performance
@
<<svm.radial.tuning, echo=FALSE, cache=TRUE, fig.cap="Wykres błędu klasyfikacji dla SVM z jądrem radialnym w zależności od parametrów kosztu i gamma.">>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="radial",
  cost=1.5^(-5:10), gamma=1.5^(-10:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)

ggplot(svm.tuning$performances, aes(x=cost, y=gamma, fill=error)) + 
  scale_y_log10() + scale_x_log10() +geom_tile() + 
  scale_fill_viridis_c() +
  labs(x="koszt", y="gamma", fill="błąd")

kable(
  data.frame(
    svm.tuning$best.parameters,
    paste0(round(100*svm.tuning$best.performance, 2), "%")),
  col.names=c("Parametr gamma", "Parametr kosztu", "Błąd klasyfikacji"),
  row.names=FALSE,
  caption="Parametry klasyfikacji dla SVM z jądrem radialnym, minimalizujące błąd klasyfikacji."
)
@


Na wykresie \ref{fig:svm.radial.tuning}
widzimy, że dobór parametróW może mieć
istotny wpływ na dokładność klasyfikacji.
Mimo tego, nie udało nam się znacznie
poprawić dokładności względem klasyfikatora
z domyślnymi parametrami (tab. \ref{tab:svm.radial.tuning}).


\subsection{Wnioski}

\end{document}