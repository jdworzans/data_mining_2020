\documentclass[12pt, a4paper]{article}
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}
<<ustawienia_globalne, echo=FALSE, warning=FALSE>>=
library(knitr)
opts_chunk$set(
  fig.path='figure/',
  fig.align='center',
  fig.pos='H',
  fig.width=5,
  fig.height=4)
@
\begin{document}

\title{Raport nr 4}
\author{Emilia Kowal [249716], Jakub Dworzański [249703]}
\maketitle
\tableofcontents

\section{Zaawansowane metody klasyfikacji}

% Zad. 1
<<ustalenie.ziarna, echo=FALSE>>=
set.seed(12062020)
@

<<biblioteki, echo=FALSE, eval=TRUE, warning=FALSE, results=FALSE, message=FALSE>>=
library(adabag)
library(e1071)
library(ipred)
library(mlbench)
library(randomForest)
library(rpart)
library(rpart.plot)
@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krótki opis zagadnienia}

W tym ćwiczeniu, będziemy kontynuowali analizę zbioru danych o odłamkach szkła.
<<przykladowe_dane, echo=FALSE>>=
data(Glass)
kable(head(Glass), caption="Przykładowe dane", format="latex")
@
<<przykladowe_dane_pokaz ,eval=FALSE>>=
data(Glass)
head(Glass)
@

% https://archive.ics.uci.edu/ml/datasets/glass+identification

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opis eksperymentów/analiz}

Tym razem skorzystamy z bardziej zaawansownych metod.
Rozpoczniemy od zastosowania rodzin klasyfikatorów.
Skupimy się na algorytmach bagging, boosting oraz random forest.

Postaramy się odkryć, jak zadziałają te algorytmy.
Ponadto, porównamy ich dokładność z poprzednimi wynikami.
Sprawdzimy również, jak złożoność algorytmu wpływa na
czas potrzebny do zbudowania modelu.

Następnie, wykorzystamy algorytm SVM -
maszynę wektorów podpierających.
Wykorzystując różne funkcje jądrowe
oraz badając przestrzenie parametrów,
postaramy się odkryć, czy ten algorytm
obniży błąd klasyfikacji.

Tym razem, aby ocenić
jakość predykcji,
będziemy korzystać
z 5-, sprawdzianu krzyżowego,
metody \emph{bootstrap} oraz \emph{632+}.

Zastosujemy do tego następujące funkcje pomocnicze,
odpowiadające za uczenie modelu oraz estymację błędów.

<<funkcje.pomocnicze>>=
mypredict <- function(model, newdata) predict(model, newdata, type="class")
adaboost.predict <- function(model, newdata){
  return(as.factor(predict(model, newdata)$class))
}
nasz.errorest <- function(formula, data, model, predict=mypredict, ...){
  cv.5 <- errorest(
    formula=formula, data=data, model=model, predict=predict,
    estimator="cv", est.para=control.errorest(k = 5), ...
  )$error
  bootstrap <- errorest(
    formula=formula, data=data, model=model, predict=predict,
    estimator="boot", est.para=control.errorest(nboot = 5), ...
  )$error
  err632plus <- errorest(
    formula=formula, data=data, model=model, predict=predict,
    estimator="boot", est.para=control.errorest(nboot = 5), ...
  )$error
  return(data.frame(
    paste0(
      round(100*c(cv.5, bootstrap, err632plus), 2),
      "%"
    ),
    row.names=c(
      "CV5",
      "bootstrap (5 powtórzeń)",
      "632+ (5 powtórzeń)"
    )
  ))
}
wyniki <- data.frame(row.names=c(
      "CV5",
      "bootstrap (5 powtórzeń)",
      "632+ (5 powtórzeń)"
    ))
@


\subsection{Rodziny klasyfikatorów}

Aby mieć punkt odniesienia podczas oceny wyników,
skorzystamy z drzewa decyzyjnego, które będzie
wykorzystywane jako klasyfikator bazowy w tym ćwiczeniu.

Przypomnijmy, że w pierwszej części analizy,
najlepszy rezultat osiągneliśmy poprzez tworzenie
drzewa na podstawie 6 najważniejszych zmiennych
oraz przycinanie go po konstrukcji za pomocą
metody jednego odchylenia.
<<waznosc.zmiennych.drzewo.pokaz, eval=FALSE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
waznosc.drzewo <- drzewo$variable.importance
waznosc.drzewo
@
<<waznosc.zmiennych.drzewo, echo=FALSE, cache=TRUE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
waznosc.drzewo <- drzewo$variable.importance
kable(waznosc.drzewo, col.names="Ważność zmiennej", format="latex",
      caption="Ważność zmiennych na podstawie konstrukcji drzewa klasyfikacyjnego")
@

<<poprzednie.wyniki.pokaz, eval=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["drzewo"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
)
wyniki["drzewo"]
@
<<poprzednie.wyniki, echo=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["drzewo"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
)
kable(
  wyniki["drzewo"],
  col.names=c("Błąd klasyfikacji"),
  caption="Najlepsze wyniki dla drzewa klasyfikacyjnego, otrzymane podczas pierwszej części analizy."
)
@

<<drzewo.wykres.pokaz, eval=FALSE>>=
drzewo <- nasze.drzewo(Type ~ RI + Al + Ca + Mg + Na + Ba, Glass)
rpart.plot(drzewo)
@
<<drzewo.wykres, echo=FALSE, fig.cap="Przykładowe drzewo klasyfikacyjne", warning=FALSE>>=
drzewo <- nasze.drzewo(Type ~ RI + Al + Ca + Mg + Na + Ba, Glass)
rpart.plot(drzewo, extra=0, yes.text="tak", no.text="nie")
@

Na rys. \ref{fig:drzewo.wykres} możemy zobaczyć
przykładowe drzewo decyzyjne, stworzone tą metodą
na podstawie całego zbioru danych.

Po zapoznaniu się z dotychczasowymi wynikami,
sprawdzimy, jak z klasyfikacją
poradzą sobie komitety klasyfikatorów.

<<wyniki.komitety, cache=TRUE>>=
wyniki["bagging"] <- nasz.errorest(
  Type ~ ., Glass, ipred::bagging
)
wyniki["boosting"] <- nasz.errorest(
  Type~., Glass, adabag::boosting, predict=adaboost.predict
)
wyniki["random.forest"] <- nasz.errorest(
  Type~., Glass, randomForest
)
@
<<porownanie.wynikow, echo=FALSE>>=
kable(
  wyniki[c("drzewo", "bagging", "random.forest", "boosting")],
  col.names=c("drzewo", "bagging", "random forest", "boosting"),
  caption="Porównanie błędów klasyfikacji."
)
@

W tabeli \ref{tab:porownanie.wynikow}
widzimy, że zastosowanie
algorytmów typu \emph{ensemble},
pozwala na znaczną poprawę wyników
(względem klasyfikatora bazowego - drzewa klasyfikacyjnego)
nawet dla komitetów z domyślnymi parametrami.

Ponadto, różnice błędu klasyfikacji
pomiędzy poszczególnymi komitetami,
są stosunkowo niewielkie
(rzędu 2-3 punktów procentowych).

Niestety, wraz ze wzrostem dokładności,
zdecydowanie wzrasta również czas,
potrzebny na trening komitetu klasyfikatorów.
Sprawdzimy teraz, jak bardzo wydłuża się
czas potrzebny do zbudowania poszczególnych
modeli.

<<czas.budowy.pokaz, eval=FALSE>>=
czas<- data.frame(row.names=c("czas"))

start <- Sys.time()
rpart(Type~., Glass)
stop <- Sys.time()
czas.drzewo <- as.numeric(stop-start)
czas["drzewo"] <- 1

start <- Sys.time()
randomForest(Type~., Glass)
stop <- Sys.time()
czas["random.forest"] <- as.numeric(stop-start) / czas.drzewo

start <- Sys.time()
ipred::bagging(Type~., Glass)
stop <- Sys.time()
czas["bagging"] <- as.numeric(stop-start) / czas.drzewo

start <- Sys.time()
boosting(Type~., Glass)
stop <- Sys.time()
czas["boosting"] <- as.numeric(stop-start) / czas.drzewo

czas
@

<<czas.budowy, echo=FALSE, cache=TRUE, results=FALSE>>=
czas<- data.frame(row.names=c("czas"))

start <- Sys.time()
rpart(Type~., Glass)
stop <- Sys.time()
czas.drzewo <- as.numeric(stop-start)
czas["drzewo"] <- 1

start <- Sys.time()
randomForest(Type~., Glass)
stop <- Sys.time()
czas["random.forest"] <- as.numeric(stop-start) / czas.drzewo

start <- Sys.time()
ipred::bagging(Type~., Glass)
stop <- Sys.time()
czas["bagging"] <- as.numeric(stop-start) / czas.drzewo

start <- Sys.time()
boosting(Type~., Glass)
stop <- Sys.time()
czas["boosting"] <- as.numeric(stop-start) / czas.drzewo
@
<<czas.budowy.prezentacja, echo=FALSE, cache=TRUE>>=
kable(
  data.frame(
    paste0(round(100 * czas, 2), "%"),
    row.names=c("drzewo", "las losowy", "bagging", "boosting")
  ),
  col.names=c("czas"),
  caption="Czas konstrukcji względem czasu konstrukcji drzewa decyzyjnego."
)
@

W tabeli \ref{tab:czas.budowy.prezentacja}
widzimy, że faktycznie czas potrzebny
na zbudowanie komitetu klasyfikatorów
może być istotnie dłuższy niż czas
potrzebny na zbudowanie drzewa.

Korzystając z konstrukcji lasu losowego,
możemy też wyznaczyć ważność
cech, podobnie jak dla pojedynczego
drzewa klasyfikacyjnego.

<<waznosc.zmiennych.porownanie.pokaz, eval=FALSE>>=
model.rf <- randomForest(Type~., Glass)
model.rf$importance
porownanie <- data.frame(
  waznosc.drzewo,
  model.rf$importance
)
names(porownanie) <- c("drzewo", "las.losowy")
ggplot(
  porownanie,
  aes(x=drzewo, y=las.losowy, label=rownames(porownanie))
) + geom_text()
@
<<waznosc.zmiennych.porownanie, echo=FALSE, cache=TRUE, fig.cap="Porównanie ważności zmiennych">>=
model.rf <- randomForest(Type~., Glass)
kable(model.rf$importance, col.names="Ważność zmiennej", format="latex",
      caption="Ważność zmiennych na podstawie konstrukcji lasu losowego.")

porownanie <- data.frame(
  waznosc.drzewo,
  model.rf$importance
)
names(porownanie) <- c("drzewo", "las.losowy")

ggplot(
  porownanie,
  aes(x=drzewo, y=las.losowy, label=rownames(porownanie))
) + geom_text() +
  labs(x="Ważność (drzewo)", y="Ważność (las losowy)")
@

W tabeli \ref{tab:waznosc.zmiennych.porownanie}
widzimy ważnośćm przypisaną poszczególnym cechom,
na podstawie konstrukcji lasu losowego.
Natomiast na wykresie \ref{fig:waznosc.zmiennych.porownanie}
możemy zobaczyć, że istotność cech
otrzymana na podstawie drzewa klasyfikacyjnego
częściowo pokrywa się z istotnością
na podstawie lasu losowego.
W szczególności, zbiór 5 najważniejszych cech
pokrywa się ze sobą dla obu "rankingów".
Ponadto, oba sposoby wskazały, że
zawartość żelaza jest najmniej istotną z cech.



\subsection{Metoda wektorów nośnych (SVM)}

Analizę algorytmu SVM rozpoczniemy
od sprawdzenia błędu klasyfikacji
dla domyślnych parametrów,
korzystając z różnych funkcji jądrowych.

<<svm.wstep.pokaz, eval=FALSE>>=
wyniki["linear SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="linear")
wyniki["polynomial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="polynomial")
wyniki["radial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="radial")

wyniki
@

<<svm.wstep, echo=FALSE, eval=TRUE, cache=TRUE>>=
wyniki["linear SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="linear")
wyniki["polynomial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="polynomial")
wyniki["radial SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="radial")
wyniki["sigmoid SVM"] <- nasz.errorest(Type~., Glass, svm, kernel="sigmoid")
kable(
  wyniki[c("linear SVM", "polynomial SVM", "radial SVM", "sigmoid SVM")],
  col.names=c("liniowe", "wielomianowe", "radialne", "sigmoidalne"),
  caption="Porównanie SVM z wykorzystaniem domyślnych parametrów i różnych funkcji jądrowych."
)
@

W tabeli \ref{tab:svm.wstep} widzimy,
że dla domyślnych parametrów,
najlepiej radzą sobie
SVM z liniową funkcją jądrową
oraz SVM z radialną funkcją jądrową.
W tej samej tabeli,
widzimy również,
że wybór funkcji jądrowej
w sposób znaczny
wpływa na dokładność klasyfikatora.
Możemy jednak przypuszczać, że ma
to również związek z większą wrażliwością
na dobór parametrów dla SVM
z wielomianową lub sigmoidalną funkcją jądrową.


Postaramy się teraz sprawdzić,
jaki wpływ na klasyfikację,
przy pomocy SVM z jądrem liniowym,
ma parametr kosztu \emph{C}.

<<svm.linear.tuning.plot.pokaz, eval=FALSE>>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="linear", cost=2^(-4:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)
ggplot(svm.tuning$performances, aes(x=cost, y=error)) + geom_point()
svm.tuning$best.parameters
svm.tuning$best.performance
@
<<svm.linear.tuning.plot, echo=FALSE, cache=TRUE, fig.cap="Wykres błędu klasyfikacji w zależności od parametru kosztu dla SVM z liniową funkcją jądrową.">>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="linear", cost=2^(-4:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)

ggplot(svm.tuning$performances,
       aes(x=cost, y=error)
       ) +
  geom_point() +
  labs(x="Parametr kosztu", y="Błąd klasyfikacji")
kable(
  data.frame(
    svm.tuning$best.parameters,
    paste0(round(100*svm.tuning$best.performance, 2), "%")),
  col.names=c("Parametr kosztu", "Błąd klasyfikacji"),
  row.names=FALSE,
  caption="Parametry klasyfikacji dla SVM z jądrem liniowym, minimalizujące błąd klasyfikacji."
)
@

Na rysunku \ref{fig:svm.linear.tuning.plot}
widzimy, że
błąd klasyfikacji zmienia się
w zależności od parametru kosztu.
Spośród zbadanych przez nas parametrów,
estymowany błąd klasyfikacji
osiąga minimum dla $C=8`$.
Otrzymujemy wtedy wynik 36.61\%
(tab. \ref{tab:svm.linear.tuning.plot}).

Widzimy, że nie doszło do znacznego
zwiększenia dokładności klasyfikacji
względem domyślnych parametrów.
Dlatego też, spróbujemy teraz
"dostroić" parametry dla SVM
z radialną funkcją jądrową,
ponieważ może być on
bardziej wrażliwy na
dobór parametrów.

<<svm.radial.tuning.pokaz, eval=FALSE>>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="radial",
  cost=1.5^(-5:10), gamma=1.5^(-10:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)
ggplot(svm.tuning$performances, aes(x=cost, y=gamma, fill=error)) + 
  geom_tile() + scale_y_log10() + scale_x_log10()
svm.tuning$best.parameters
svm.tuning$best.performance
@
<<svm.radial.tuning, echo=FALSE, cache=TRUE, fig.cap="Wykres błędu klasyfikacji dla SVM z jądrem radialnym w zależności od parametrów kosztu i gamma.">>=
svm.tuning <- tune.svm(
  Type~., data=Glass, kernel="radial",
  cost=1.5^(-5:10), gamma=1.5^(-10:7),
  tunecontrol=tune.control(sampling="boot", nboot=25)
)

ggplot(svm.tuning$performances, aes(x=cost, y=gamma, fill=error)) + 
  scale_y_log10() + scale_x_log10() +geom_tile() + 
  scale_fill_viridis_c() +
  labs(x="koszt", y="gamma", fill="błąd")

kable(
  data.frame(
    svm.tuning$best.parameters,
    paste0(round(100*svm.tuning$best.performance, 2), "%")),
  col.names=c("Parametr gamma", "Parametr kosztu", "Błąd klasyfikacji"),
  row.names=FALSE,
  caption="Parametry klasyfikacji dla SVM z jądrem radialnym, minimalizujące błąd klasyfikacji."
)
@


Na wykresie \ref{fig:svm.radial.tuning}
widzimy, że dobór parametrów może mieć
istotny wpływ na dokładność klasyfikacji.
Mimo tego, nie udało nam się znacznie
poprawić dokładności względem klasyfikatora
z domyślnymi parametrami (tab. \ref{tab:svm.radial.tuning}).


\subsection{Wnioski}

Na podstawie przeprowadzonych eksperymentów,
możemy stwierdzić, że bardziej zaawansowane
metody klasyfikacji pozwalają na osiągnięcie
dużo niższego błędu klasyfikacji.
W przypadku komitetów klasyfikatorów,
osiągaliśmy obniżenie błędu klasyfikacji
o niemalże 50\%.

Widzieliśmy również, że wyższa skuteczność
idzie w parze z większą złożonością algorytmu,
przez co budowanie modeli może się znacznie
wydłużyć.


W przypadku SVM, poprawa wyników nie była
tak znacząca. Ten problem może być spowodowany
przez różne czynniki.
Wpływ na wysokość błędu predykcji może mieć
to, że SVM jest algorytmem do klasyfikacji
binarnej, a wykorzystywanie go do problemów
wieloklasowych, może pogarszać jego przewagę
nad pozostałymi metodami.
Ponadto, problemem może być odnalezienie
lokalnego minimum w przestrzeni parametrów
modelu.
Mimo wyników porównywalnych z wcześniej
poznanymi klasyfikatorami,
mogliśmy się przekonać, że w przypadku SVM
dobór parametrów jest bardzo istotny i ma
duży wpływ na błąd klasyfikacji.

% Zad. 2

<<biblioteki.2, echo=FALSE, eval=TRUE, warning=FALSE, results=FALSE, message=FALSE>>=
library(mlbench)
library(MASS)
library(cluster)
library(factoextra)
library(ggplot2)
library(e1071)
library(clValid)
library(mclust)
@

\section{Analiza skupień - algorytmy grupujące i hierarchiczne}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krótki opis zagadnienia}

W tej sekcji będziemy zajmować się analizą skupień z wykorzystaniem algorytmów grupujących oraz hierarchicznych. Analizy dokonujemy na zbiorze Glass z biblioteki datasets.
<<przykladowe_dane.2, echo=FALSE>>=
data(Glass)
kable(head(Glass), caption="Przykładowe dane", format="latex")
@
<<przykladowe_dane_pokaz.2 ,eval=FALSE>>=
data(Glass)
head(Glass)
@

% https://archive.ics.uci.edu/ml/datasets/glass+identification

Dane (tabela: \ref{tab:przykladowe_dane}) składają się z informacji, dotyczących 214 odłamków.
Każdy z rekordów składa się ze współczynnika załamania światła ({\verb+RI+})
oraz stężenia różnych pierwiastków (w odpowiadających tlenkach)
w badanych próbkach.
Oprócz tego, posiadamy informacje o typie szkła,
z którego odłamkiem mamy do czynienia.
Razem, mamy 7 takich klas, z których tylko 6 występuje w danych.
Łącznie, dla każdej próbki występuje 10 cech.
Ponadto, w zbiorze nie ma wartości brakujących.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opis eksperymentów/analiz}

\begin{itemize}
\item W pierwszym kroku zastosujemy algorytm grupujący PAM oraz algorytm hierarchiczny AGNES.
\item Następnie zobrazujemy wyniki algorytmów grupujących wykorzystując wykresy rozrzutu. Aby przedstawić rezultaty na wykresie dwuwymiarowym, zastosujemy algorytm PCA.
\item Dla algorytmów hierarchicznych porównamy dendrogramy dla różnych metod łączenia skupień.
\item Ocenimy jakość grupowania z wykorzystaniem wskaźników zewnętrznych, takich jak macierz kontyngencji oraz wskaźników zewnętrznych, np. indeksu silhouette. 
\item Następnie postaramy się dobrać optymalną liczbę klas dla zbioru danych Glass.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wyniki}

Usuwamy ze zbioru Glass zmienną grupującą Type, zawierającą etykietki klas.
<<>>=
glass.cechy <- Glass[, -10]
glass.real.etykiety <- Glass[, 10]
@

Zbiór Glass zawiera 214 wierszy, zatem nie ma konieczności losowania podzbioru w celu zredukowania czasu działania algorytmów. \\
Przed wyznaczeniem macierzy niepodobieństw nie standaryzujemy danych. Podczas analizy chemicznej, do rozstrzygnięcia o typie szkła, istotnym jest, który z pierwiastków dominuje w składzie. 
Następnie wyznaczamy macierz niepodobieństw z metryką euklidesową dla nieustandaryzowanych danych.

<<>>=
diss.mtrx.glass <- daisy(glass.cechy)
@

Wizualizacja macierzy odmienności po uporządkowaniu dla cech ze zbioru \textit{Glass}:

<<echo=FALSE, fig.cap="Macierz odmienności po uporządkowaniu">>=
fviz_dist(diss.mtrx.glass, order = TRUE)
@

\subsubsection{Algorytm PAM (Partitioning Around Medoids)}

W pierwszej kolejności do analizy skupień wykorzystamy algorytm grupujący PAM, przyjmując liczbę klas k=6 zgodną z rzeczywistymi etykietkami klas.

<<>>=
glass.pam <- pam(x=diss.mtrx.glass, diss=TRUE, k=6)
@

<<medoidy_pam_k_6, echo=FALSE>>=
kable(t(glass.pam$medoids), format="latex", caption="Medoidy reprezentujące klastry dla k=6")
@

W tabeli \ref{tab:medoidy_pam_k_6} widzimy medoidy reprezentujące klastry dla 6 klas.

Tworzymy wizualizację z wykorzystaniem algorytmu PCA za pomocą funkcji \textit{fviz\_cluster} z biblioteki \textit{factoextra}.

<<clusters_vis_pam_6, echo=FALSE, fig.cap="Wizualizacja PAM z wykorzystaniem algorytmu PCA">>=
glass.pam.original <- glass.pam
glass.pam$data <- glass.cechy
fviz_cluster(glass.pam) 
@

Skupiska nie są dobrze odseparowane. \\
Skuteczność algorytmu PAM dla zbioru danych \textit{Glass} sprawdzimy również wizualizując wynik wskaźnika \textit{silhouette}.

<<silhouette_pam_6, echo=FALSE, fig.cap="Wskaźnik silhouette dla algorytmu PAM">>=
plot(glass.pam)
@

Zbadamy również poziom zgodności otrzymanych wyników z rzeczywistymi etykietami obiektów wykorzystując tabelę kontyngencji. Zgodnie z tabelą \ref{tab:tab.pam.6} dostajemy zgodność ok. 48%.

<<tab.pam.6, echo=FALSE>>=
cont_mtrx_pam6 <- table(glass.pam$clustering, glass.real.etykiety)
kable(cont_mtrx_pam6, format="latex", caption="Tabela kontyngencji dla PAM i k=6")
@

Dla porównania wyznaczyliśmy również macierz kontyngencji dla danych ustandaryzowanych i otrzymaliśmy zgodność na poziomie ok. 43\% (tabela: \ref{tab.pam.s.6}), zatem o 5 punktów procentowych gorszą od wyniku dla danych nieustandaryzowanych. W związku z tym w dalszej części raportu pracujemy już tylko na zbiorze nieskalowanym. 
 
<<tab.pam.s.6, echo=FALSE>>=
glass.cechy.scaled <- scale(glass.cechy)
diss.mtrx.glass.s <- daisy(glass.cechy.scaled)
glass.pam.s <- pam(x=diss.mtrx.glass.s, diss=TRUE, k=6)
cont_mtrx_pam6.s <- table(glass.pam.s$clustering, glass.real.etykiety)
kable(cont_mtrx_pam6.s, format="latex", caption="Tabela kontyngencji dla PAM na danych ustandaryzowanych i k=6")
@

Następnie wyznaczyliśmy sugerowaną liczbę klas z pomocą algorytmu \textit{fviz\_nbclust} w oparciu o wskaźnik \textit{silhouette}.

<<echo=FALSE, fig.cap="Sugerowana liczba klas dla algorytmu PAM">>=
fviz_nbclust(glass.cechy, pam, method = "silhouette")
@

Dostajemy informację, iż optymalna liczba klas wynosi 3. Wnioskujemy, że niektóre typy szkła mogą mieć zbliżony skład, a w konsekwencji zostają przypisane do skupienia, gdzie poszczególne wyniki mają podobne właściwości. \\

Powtarzamy analizę z wykorzystaniem algorytmu PAM tym razem dla 3 klas.

<<echo=FALSE, fig.cap="PAM bez standaryzacji dla 3 klas.">>=
glass.pam.3 <- pam(x=diss.mtrx.glass, diss=TRUE, k=3)
plot(glass.pam.3)
@

Otrzymujemy następującą tabelę kontyngencji: \ref{tab:tab.pam.3} oraz poziom zgodności 50\%. 

<<tab.pam.3, echo=FALSE>>=
cont_mtrx_pam3 <- table(glass.pam.3$clustering, glass.real.etykiety)
kable(cont_mtrx_pam3, format="latex", caption="Tabela kontyngencji dla PAM i k=3")
matchClasses(cont_mtrx_pam3)
@


\subsubsection{Algorytm AGNES (Aglomerative Nesting)}

W tej sekcji do analizy skupień wykorzystamy algorytm hierarchiczny AGNES. Wyniki działania przeanalizujemy dla różnych metod łączenia skupień. Na początku przyjmujemy rzeczywistą liczbę klas równą 6.

<<agnes.sing.6>>=
glass.agnes.single <- agnes(x=diss.mtrx.glass, diss=TRUE, method="single")
@

Dendrogram dla algorytmu AGNES z metodą \textit{single}.
<<echo=FALSE, fig.cap="AGNES z metodą single dla k=6", fig.height=6, fig.width=7>>=
fviz_dend(glass.agnes.single, k=6)
@

<<agnes.avg.6>>=
glass.agnes.avg <- agnes(x=diss.mtrx.glass, diss=TRUE, method="average")
@

Dendrogram dla algorytmu AGNES z metodą \textit{average}.
<<echo=FALSE, fig.cap="AGNES z metodą average dla k=6", fig.height=6, fig.width=7>>=
fviz_dend(glass.agnes.avg, k=6)
@

<<agnes.complete.6>>=
glass.agnes.complete <- agnes(x=diss.mtrx.glass, diss=TRUE, method="complete")
@

Dendrogram dla algorytmu AGNES z metodą \textit{complete}.
<<echo=FALSE, fig.cap="AGNES z metodą complete dla k=6", fig.height=6, fig.width=7>>=
fviz_dend(glass.agnes.complete, k=6)
@

<<agnes.ward.6>>=
glass.agnes.ward <- agnes(x=diss.mtrx.glass, diss=TRUE, method="ward") 
@

Dendrogram dla algorytmu AGNES z metodą \textit{Warda}.
<<echo=FALSE, fig.cap="AGNES z metodą Warda dla k=6", fig.height=6, fig.width=7>>=
fviz_dend(glass.agnes.ward, k=6)
@

Na podstawie samych wykresów można wywnioskować, że najlepszy podział na skupienia wyznacza metoda \textit{Warda} oraz \textit{complete}. \\
Hipotezę sprawdzimy wykorzystując wskaźnik wewnętrzny \textit{silhouette} i tabel kontyngencji oraz porównując wyniki wskaźników dla każdej z metod łączenia skupień.

<<fig.cap="Wskaźnik silhouette dla metody single">>=
cut.glass.agnes.sing <- cutree(glass.agnes.single, k=6)
sil.glass.agnes.sing <- silhouette(cut.glass.agnes.sing, dist=diss.mtrx.glass)
 fviz_silhouette(sil.glass.agnes.sing)
@

Macierz kontyngencji dla metody łączenia skupień \textit{single}: \ref{tab:tab.agnes.sing}. Otrzymujemy zgodność na poziomie: 37,38\%.

<<tab.agnes.sing, echo=FALSE, warning=FALSE, message=FALSE>>=
 tab.agnes.sing <- table(cut.glass.agnes.sing, glass.real.etykiety)
 kable(tab.agnes.sing, format="latex", caption="Tabela kontyngencji dla metody single")
 #matchClasses(tab.agnes.sing)
@

<<echo=FALSE, fig.cap="Wskaźnik silhouette dla metody average">>=
cut.glass.agnes.avg <- cutree(glass.agnes.avg, k=6)
sil.glass.agnes.avg <- silhouette(cut.glass.agnes.avg, dist=diss.mtrx.glass)
  fviz_silhouette(sil.glass.agnes.avg)
@

Macierz kontyngencji dla metody łączenia skupień \textit{average}: \ref{tab:tab.agnes.avg}. Otrzymujemy zgodność na poziomie: 38.32 \%.

<<tab.agnes.avg, echo=FALSE, warning=FALSE, message=FALSE>>=
 tab.agnes.avg <- table(cut.glass.agnes.avg, glass.real.etykiety)
  kable(tab.agnes.avg, format="latex", caption="Tabela kontyngencji dla metody average")
 #matchClasses(tab.agnes.avg)
@

<<echo=FALSE, fig.cap="Wskaźnik silhouette dla metody complete">>=
cut.glass.agnes.compl <- cutree(glass.agnes.complete, k=6)
sil.glass.agnes.compl <- silhouette(cut.glass.agnes.compl, dist=diss.mtrx.glass)
fviz_silhouette(sil.glass.agnes.compl)
@

Macierz kontyngencji dla metody łączenia skupień \textit{complete}: \ref{tab:tab.agnes.comp}. Otrzymujemy zgodność na poziomie: 50\%.

<<tab.agnes.comp, echo=FALSE, warning=FALSE, message=FALSE>>=
 tab.agnes.comp <- table(cut.glass.agnes.compl, glass.real.etykiety)
 kable(tab.agnes.comp, format="latex", caption="Tabela kontyngencji dla metody complete")
 #matchClasses(tab.agnes.comp)
@

<<echo=FALSE, fig.cap="Wskaźnik silhouette dla metody Warda">>=
cut.glass.agnes.ward <- cutree(glass.agnes.ward, k=6)
sil.glass.agnes.ward <- silhouette(cut.glass.agnes.ward, dist=diss.mtrx.glass)
fviz_silhouette(sil.glass.agnes.ward)
@

Macierz kontyngencji dla metody łączenia skupień \textit{Warda}: \ref{tab:tab.agnes.ward}. Otrzymujemy zgodność na poziomie: 54.21 \%.

<<tab.agnes.ward, echo=FALSE, warning=FALSE, message=FALSE>>=
 tab.agnes.ward <- table(cut.glass.agnes.ward, glass.real.etykiety)
 kable(tab.agnes.ward, format="latex", caption="Tabela kontyngencji dla metody Warda")
 #matchClasses(tab.agnes.ward)
@
Na podstawie powyższej analizy zauważamy, że dla $k=6$ metoda \textit{Warda} uzyskuje osiąga lepsze wyniki dla wskaźnika zewnętrznego, natomiast metoda \textit{complete} powoduje, że klastry są bardziej zwarte i lepiej od siebie odseparowane.\\

W celu dokładniejszej oceny jakości grupowania wykorzystamy bibliotekę \textit{clValid}. Zbadamy skuteczność algorytmów \textit{kmeans}, \textit{diana},  \textit{PAM}, \textit{agnes} oraz \textit{clara} dla klas $k=2,...,6$ \\
Najpierw zajmiemy się wskaźnikami wewnętrznymi. Tabela \ref{tab:optimal.internal} zawiera informacje dotyczące algorytmów, dla których udało się osiągnąć najlepsze wyniki poszczególnych wskaźników.

<<optimal.internal, echo=FALSE>>=
metody <- c("pam", "agnes", "kmeans", "diana",  "clara")
zakres <- 2:6

internal.validation <- clValid(glass.cechy, nClust=zakres, clMethods=metody, validation="internal")
#summary(internal.validation)
kable(optimalScores(internal.validation), format="latex", caption="Algorytmy, dla których wskaźniki wewnętrzne są optymalne") 
@

<<internal.plots, echo=FALSE,fig.height=9, fig.width=7, fig.cap="Porównanie skuteczności algorytmów w zależności od liczby klas">>=
par(mfrow = c(3, 1))
plot(internal.validation, lwd=2)
@

Następnie zbadamy skuteczność algorytmów na wskaźnikach stabilności. Tabela \ref{tab:optimal.stability} zawiera informacje na temat algorytmów, które osiągają najlepsze wyniki dla poszczególnych wskaźników stabilności.

<<optimal.stability, echo=FALSE>>=
stability.validation <- clValid(glass.cechy, nClust=zakres, clMethods=metody, validation="stability")
#summary(stability.validation)
kable(optimalScores(stability.validation), format="latex", caption="Algorytmy, dla których wskaźniki stabilności są optymalne")
@

<<stability.plots, echo=FALSE, fig.height=9, fig.width=7, fig.cap="Porównanie skuteczności algorytmów w zależności od liczby klas">>=
par(mfrow = c(3,1))
plot(stability.validation, measure=c("APN","AD","ADM"), lwd=2)
@

\subsection{Podsumowanie}
\begin{itemize}
\item Dla zboru danych \textit{Glass}, spośród zastosowanych funkcji, najlepsze rezultaty przyniósł algorytm hierarchiczny AGNES z metodą łączenia skupień \textit{Warda}.
\item Czas działania algorytmu AGNES był dłuższy niż w przypadku algorytmu PAM. Dla zbiorów danych zawierających wiele rekordów lub cech, wyznaczanie odpowiednich skupisk z wykorzystaniem algorytmów hierarchicznych mogłoby być czasochłonne.
\item Otrzymujemy najbardziej zwarte i odseparowane od siebie klastry dla algorytmu agnes z metodą \textit{complete}.
\item Zbiór cech wykorzystany do wyznaczenia skupisk nie charakteryzuje się wysoką zdolnością dyskryminacyjną.
\end{itemize}



\end{document}