\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dodatkowe pakiety LaTeX'a
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ustawienia globalne
<<ustawienia_globalne, echo=FALSE, warning=FALSE>>=
library(knitr)
opts_chunk$set(
  fig.path='figure/',
  fig.align='center',
  fig.pos='H',
  fig.width=5,
  fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% strona tytulowa


\title{Raport nr 3}
\author{Emilia Kowal [249716], Jakub Dworzański [249703]}
\maketitle
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Zad. 1
\section{Klasyfikacja na bazie modelu regresji liniowej}

<<>>=
library(datasets)
library(ggplot2)
library(varhandle)
@


\subsection{Krótki opis zagadnienia}
W tej sekcji będziemy zajmować się klasyfikacją opartą na modelu regresji liniowej, z wykorzystaniem metody najmniejszych kwadratów.
Analizy dokonujemy na zbiorze danych \textit{iris}:

<<echo=FALSE>>=
data(iris)
attach(iris)
head(iris)
@

Zbiór danych zawiera 150 wierszy z informacjami na temat 3 gatunków irysów: \textit{setosa}, \textit{versicolor} i \textit{virginica}. Zmienne dostarczają nam wiedzy o szerokości i długości działki kielicha oraz płatka.
Dane nie zawierają obserwacji brakujących.

Po dokonaniu analizy postaramy się odpowiedzieć na pytania o:
\begin{itemize}
\item skuteczność klasyfikacji opartej na modelu regresji liniowej;
\item wpływ ilości klas na jakość predykcji;
\item maskowanie klas;
\item wpływ składników wielomianowych stopnia 2 na dokładność klasyfikacji.
\end{itemize}

\subsection{Opis eksperymentów/analiz}

\begin{itemize}
\item W pierwszym kroku, budując model klasyfikacyjny, dokonujemy podziału zbioru danych na treningowy oraz testowy w stosunku 1:5. 
\item Następnie na podstawie danych uczących kostruujemy klasyfikator i predykujemy etykiety dla zbioru treningowego i testowego.
\item Potem dokonujemy oceny jakości modelu, wyznaczamy macierz pomyłek, przeprowadzamy analizę graficzną klasyfikatora.
\item Ostatecznie badamy wpływ składników wielomianowych stopnia 2 na dokładność modelu regresji liniowej, dodając do zbioru danych cechę Petal.Length$^2$ oraz Petal.Width$*$Sepal.Length.
\end{itemize}

\subsection{Wyniki}

Na podstawie wykresu widzimy, iż poszczególne klasy zwierają tyle samo obiektów:

<<klasy, echo=FALSE, fig.cap="Przynależności do klas.">>=
ggplot(data=iris, aes(x=Species)) + 
  geom_bar() + labs(x="gatunek", y="liczebność gatunku")
@

\subsubsection{Analiza skuteczności modelu dla zbioru danych bez składników wielomianowych stopnia 2.}
Dokonujemy podziału zbioru danych na trengingowe oraz testowe:

<<train_test_split>>=
smp_size <- floor(0.8 * nrow(iris))
set.seed(43)
train_ind <- sample(seq_len(nrow(iris)), size=smp_size)
# pamiętamy, żeby uwzględnić wyraz wolny w modelu regresji
X_train <- cbind(rep(1,120),iris[train_ind, 1:4])
X_test <- cbind(rep(1,30),iris[-train_ind, 1:4])
Y_train <- iris[train_ind, 5]
Y_test <- iris[-train_ind, 5]
@


Konwertujemy zmienną kategoryczną na macierz wskaźnikową z wykorzystaniem funkcji \textit{to.dummy} z pakietu \textit{varhandle}:
<<konwersja>>=
Y_train_bin <- to.dummy(Y_train, prefix="species")
Y_test_bin <- to.dummy(Y_test, prefix="species")
@

Konstrujujemy klasyfikator regresji liniowej metodą najmniejszych kwadratów:

<<classifier>>=
B <- solve(t(X_train)%*%as.matrix(X_train)) %*% t(X_train) %*% as.matrix(Y_train_bin)
@
Dla zbioru testowego otrzymujemy następujące wyniki: \ref{tab:result_test}.

<<result_test, echo=FALSE>>=
Y_result <- as.matrix(X_test) %*% B
klasy <- levels(Species)
  maks.ind <- apply(Y_result, 1, FUN=function(x) which.max(x))
  prognozowane.etykietki <- klasy[maks.ind]
  rzeczywiste.etykietki <- Y_test
  
  conf_mtrx <- table(rzeczywiste.etykietki, prognozowane.etykietki)
  rownames(conf_mtrx) <- c("setosa.true", "versicolor.true", "virginica.true")
  kable(conf_mtrx,caption="Skuteczność estymatora dla danych testowych.", 
        format="latex")
  #sum(diag(conf_mtrx))/length(Y_test)
@
Dostajemy dokładność na poziomie: 73\%. \\
<<result_test_plot, echo=FALSE, fig.cap="Wizualizacja wyników dla zbioru testowego.", fig.height=6, fig.width=8>>=
matplot(Y_result, ylab="Predykcja")
  abline(v=c(10,20), lty=2, col="grey")
  legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@
Na wykresie \ref{fig:result_test_plot} widzimy, że dominują klasy setosa i virginica, zatem wnioskujemy, iż versicolor jest zamaskowana (zdominowana) przez pozostałe dwie klasy. \\
Następnie dokonujemy analizy zgodności dla zbioru treningowego w celu uniknięcia przetrenowania modelu. Otrzymujemy następujące rezultaty: \ref{tab:result_train}.

<<result_train, echo=FALSE>>=
Y_result.train <- as.matrix(X_train) %*% B

maks.ind.train <- apply(Y_result.train, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.train <- klasy[maks.ind.train]
rzeczywiste.etykietki.train <- Y_train

conf_mtrx.train <- table(rzeczywiste.etykietki.train, prognozowane.etykietki.train)
rownames(conf_mtrx.train) <- c("setosa.true", "versicolor.true", "virginica.true")

kable(conf_mtrx.train, caption="Skuteczność estymatora dla danych treningowych.", 
      format="latex")
#sum(diag(conf_mtrx.train))/length(Y_train)
@
Dostajemy zgodność na poziomie: 88\%, co mogłoby wskazywać na to, iż model jest przetrenowany. Aby uniknąć takiej sytuacji, w dalszej analizie należałoby przeprowadzić np. cross validation test, który dałby nam rzetelniejsze wyniki dla całego zbioru danych. Również fakt, iż w zbiorze nie mamy wielu rekordów, działa niekorzystnie na skuteczność modelu regresji. \\
Wyniki przedstawione graficznie:
<<result_train_plot, echo=FALSE, fig.cap="Wizualizacja wyników dla zbioru treningowego.", fig.height=6, fig.width=8>>=
matplot(Y_result.train, ylab="Predykcja")
  abline(v=c(40,80), lty=2, col="grey")
  legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@

\subsubsection{Analiza skuteczności modelu dla zbioru danych ze składnikami wielomianowymi stopnia 2.}
Do konstrukcji drugiego modelu regresji wykorzystujemy zbiór z wyjściowymi cechami oraz zmiennymi wielomianowymi stopnia 2. 
<<new_dataset, echo=FALSE>>=
  new_iris <- cbind(Petal.Length^2, Petal.Width^2, 
                  Sepal.Length^2, Sepal.Width^2,
                  Petal.Length*Petal.Width,
                  Petal.Width*Sepal.Length,
                  Petal.Length*Sepal.Width,
                  Petal.Length*Sepal.Length,
                  Petal.Width*Sepal.Width,
                  Sepal.Length*Sepal.Width,
                  iris)

X_train_n <- cbind(rep(1,120),new_iris[train_ind, 1:14])
X_test_n <- cbind(rep(1,30),new_iris[-train_ind, 1:14])
Y_train_n <- new_iris[train_ind, 15]
Y_test_n <- new_iris[-train_ind, 15]

# konwersja zmiennej kategorycznej na macierz wskaźnikową:
Y_train_bin_n <- to.dummy(Y_train_n, prefix="species")
Y_test_bin_n <- to.dummy(Y_test_n, prefix="species")

# konstrukcja klasyfikatora regresji liniowej metodą najmniejszych kwadratów:
B1 <- solve(t(X_train_n)%*%as.matrix(X_train_n)) %*% t(X_train_n) %*% as.matrix(Y_train_bin_n)

# wartości progonozowane:
Y_result_n <- as.matrix(X_test_n) %*% B1
@

Dla estymatora, skonstruowanego w oparciu o nowy zbiór danych, dla zbioru testowego otzymujemy następujące wyniki: \ref{tab:result_test_new}.

<<result_test_new, echo=FALSE>>=
maks.ind.n <- apply(Y_result_n, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.n <- klasy[maks.ind.n]
rzeczywiste.etykietki.n <- Y_test_n

conf_mtrx_n <- table(rzeczywiste.etykietki.n, prognozowane.etykietki.n)
rownames(conf_mtrx_n) <- c("setosa.true", "versicolor.true", "virginica.true")
kable(conf_mtrx_n, caption="Skuteczność nowego estymatora dla zbioru testowego.",format="latex")
@

Dostajemy zgodność na poziomie: 97\%, zatem otrzymujemy znacznie skuteczniejszy estymator dla zbioru danych ze składnikami wielomianowymi stopnia 2. 

<<result_test_new_plot, echo=FALSE, fig.cap="Wizualizacja wyników nowego estymatora dla zbioru testowego.", fig.height=6, fig.width=8>>=
matplot(Y_result_n, ylab="Predykcja")
abline(v=c(10,20), lty=2, col="grey")
legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@
Na podstawie wykresu \ref{fig:result_test_new_plot} zauważamy, że dla nowego zbioru danych, klasy setosa oraz virginica nie dominują w sposób znaczny klasy versicolor. Wnioskujemy, że z tego też względu otrzymujemy znacznie lepszy rezultat dla modelu regresji. \\
Zbadamy jeszcze skuteczność estymatora dla zbioru treningowego: \ref{tab:result_train_new}.

<<result_train_new, echo=FALSE>>=
Y_result.train.n <- as.matrix(X_train_n) %*% B1

maks.ind.train.n <- apply(Y_result.train.n, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.train.n <- klasy[maks.ind.train.n]
rzeczywiste.etykietki.train.n <- Y_train_n

conf_mtrx.train.n <- table(rzeczywiste.etykietki.train.n, prognozowane.etykietki.train.n)
rownames(conf_mtrx.train.n) <- c("setosa.true", "versicolor.true", "virginica.true")
kable(conf_mtrx.train.n, caption="Skuteczność nowego estymatora na zbiorze treningowym.", 
      format="latex")
@

Otrzymujemy zgodność na poziomie 98\%.

<<resutl_train_new_plot, echo=FALSE, fig.height=6, fig.width=8>>=
matplot(Y_result_n, ylab="Predykcja")
abline(v=c(40,80), lty=2, col="grey")
legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@


\subsection{Podsumowanie}
\begin{itemize}
\item Po przeprowadzeniu analiz wnioskujemy, iż model oparty na regresji liniowej jest skuteczny, kiedy zależności między zmiennymi są liniowe. Jak wiadomo, zazwyczaj jednak mamy styczność ze zmiennymi o zależnościach nieliniowych, gdzie estymator regresji może dawać niesatysfakcjonujące rezultaty. Widzimy, iż po dodaniu do zbioru danych cech wielomianowych stopnia 2, skuteczność klasyfikatora znacznie wzrasta
\item Ponadto, kiedy tworzymy model klasyfikacyjny dla większej liczby klas, regresja liniowa jest narażona na problem maskowania klas, który również wpływa niekorzystnie na wyniki estymatora.
\item Tworząc model regresji należy pamiętać o zapobiaganiu jego przetrenowania wykorzystując metody takie, jak \textit{cross-validation}  lub \textit{bagging}.
\end{itemize}
Podsumowując regresja liniowa zakłada, iż w zbiorze zmienne mają liniowe zależności. Pracując nad bardziej złożonym problemem klasyfikacyjnym, dla dużej liczby klas, wybór odpowiednich składników wielomianowych może być czasochłonne, a przez to nieefektywne. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Zad. 2
<<ustalenie.ziarna, echo=FALSE>>=
set.seed(14052020)
@

<<biblioteki, echo=FALSE>>=
library(mlbench)
library(ipred)
library(rpart)
library(e1071)
@

\section{Porównanie metod klasyfikacji}

\subsection{Opis danych}
<<echo=FALSE>>=
data(Glass)
str(Glass)
# more info:
# https://archive.ics.uci.edu/ml/datasets/glass+identification
# 1. Id number: 1 to 214
# 2. RI: refractive index
# (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)
# 3. Na: Sodium 
# 4. Mg: Magnesium
# 5. Al: Aluminum
# 6. Si: Silicon
# 7. K: Potassium
# 8. Ca: Calcium
# 9. Ba: Barium
# 10. Fe: Iron
# 11. Type of glass: (class attribute)
# -- 1 building_windows_float_processed
# -- 2 building_windows_non_float_processed
# -- 3 vehicle_windows_float_processed
# -- 4 vehicle_windows_non_float_processed (none in this database)
# -- 5 containers
# -- 6 tableware
# -- 7 headlamps
@



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krótki opis zagadnienia}

W tym ćwiczeniu, mamy do czynienia ze zbiorem danych o odłamkach szkła.
<<przykladowe_dane, echo=FALSE>>=
kable(head(Glass), caption="Przykładowe dane", format="latex")
@
<<przykladowe_dane_pokaz ,eval=FALSE>>=
head(Glass)
@


Dane składają się z informacji, dotyczących 214 odłamków.
Każdy z rekordów składa się ze współczynnika załamania światła ({\verb+RI+})
oraz stężenia różnych pierwiastków (w odpowiadających tlenkach)
w badanych próbkach.
Oprócz tego, posiadamy informacje o typie szkła,
którego 
Łącznie, dla każdej próbki występuje 13 cech.
Ponadto, w zbiorze nie ma wartości brakujących.


Aby ocenić jakość predykcji, będziemy korzystać
z 5-, 10-krotnego sprawdzianu krzyżowego
oraz sprawdzianu krzyżowego typu leave-one-out
(\emph{5-, 10-, leave-one-out Cross Validation}).
Ponadto, wykorzystamy metodę \emph{bootstrap} oraz \emph{632+}.

Wykorzystamy do tego następujące funkcje pomocnicze,
odpowiadające za uczenie modelu oraz estymację błędów.

<<funkcje.pomocnicze>>=
mypredict <- function(model, newdata) predict(model, newdata, type="class")
nasz.errorest <- function(formula, data, model){
  cv.10 <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = 10)
  )$error
  cv.5 <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = 5)
  )$error
  leave.one.out <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = nrow(data))
  )$error
  bootstrap <- errorest(
    formula=formula, data=data, model=model,
    predict=mypredict, estimator="boot"
  )$error
  err632plus <- errorest(
    formula=formula, data=data, model=model,
    predict=mypredict, estimator="632plus"
  )$error
  return(data.frame(
    paste0(
      round(100*c(cv.5, cv.10, leave.one.out, bootstrap, err632plus), 2),
      "%"
    ),
    row.names=c(
      "CV5", "CV10", "CV leave-one-out",
      "bootstrap (25 powtórzeń)",
      "632+ (25 powtórzeń)"
    )
  ))
}
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opis eksperymentów/analiz}

Na pewno:
k-NN
drzewka
naiwny klasyfikator bayesowski


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wyniki}

W tej części zamieszczamy uzyskane wyniki, w szczególności fragmenty R-kodów wraz z tabelami i wykresami.

\subsubsection{Modele z domyślnymi wartościami dla wszystkich cech}
Nasze badania rozpoczniemy od estymacji błędu klasyfikacji dla modeli
z domyślnymi parametrami i bez wcześniejszych przekształceń danych.
Pozwoli to nam później ocenić ewentualną poprawę wyników.

<<surowe.porowananie.pokaz, eval=FALSE>>=
wyniki <- nasz.errorest(Type ~ ., Glass, ipredknn)
wyniki["drzewo"] <- nasz.errorest(Type ~ ., Glass, rpart)
wyniki["nb"] <- nasz.errorest(Type ~ ., Glass, naiveBayes)

wyniki
@

<<surowe.porownanie, echo=FALSE, cache=TRUE>>=
wyniki <- nasz.errorest(Type ~ ., Glass, ipredknn)
names(wyniki) <- c("k.nn")
wyniki["drzewo"] <- nasz.errorest(Type ~ ., Glass, rpart)
wyniki["nb"] <- nasz.errorest(Type ~ ., Glass, naiveBayes)

kable(
  wyniki,
  format="latex",
  col.names=c("k-NN", "drzewo klasyfikacyjne", "naiwny klasyfikator bayesowski"),
  caption="Porównanie błędu klasyfikacji dla modeli z domyślnymi parametrami.",
  digits=2,
  align="r"
)
@

W tabeli \ref{tab:surowe.porownanie} widzimy, że dla domyślnych parametrów,
z całymi danymi najlepiej radzą sobie metoda k-najbliższych sąsiadów oraz
drzewa klasyfikacyjne. Można również zauważyć, że naiwny klasyifkator bayesowski
zadziałał w tym przypadku zdecydowanie gorzej.

Postaramy się znaleźć odpowiednie podzbiory cech oraz parametry dla poszczególnych
modeli, aby poprawić otrzymane rezultaty.

\subsubsection{Analiza opisowa}


\subsubsection{Metoda k-najbliższych sąsiadów}

\subsubsection{Drzewo klasyfikacyjne}
Skorzystamy z metody jednego odchylenia standardowego,
przedstawionej na wykładzie, aby zmniejszać złożoność
drzewa po uczeniu.
<<przyciete.drzewo.pokaz, eval=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["przyciete.drzewo"] <- nasz.errorest(Type ~ ., Glass, nasze.drzewo)
wyniki[c("drzewo", "przyciete.drzewo")]
@

<<przyciete.drzewo, echo=FALSE, cache=TRUE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["przyciete.drzewo"] <- nasz.errorest(Type ~ ., Glass, nasze.drzewo)
kable(
  wyniki[c("drzewo", "przyciete.drzewo")],
  col.names=c("drzewo", "przycięte drzewo"),
  format="latex",
  caption="Sprawdzenie poprawy błędu klasyfikacji po przycięciu drzewa."
)
@


W tabeli \ref{tab:przyciete.drzewo} widzimy, że przycięcie drzewa za pomocą
metody 1 odchylenia standardowego pozwala nie zmienia znacząco wyników.
Możemy jednak przypuszczać, że przycięte drzewo ma lepsze właściwości
uogólniające.

Możemy również spróbować wybrać najważniejsze zmienne, na podstawie
konstrukcji drzewa klasyfikacyjnego, a następnie skonstruować model
na podstawie tych cech.

<<waznosc.zmiennych.pokaz, eval=FALSE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
drzewo$variable.importance
@
<<waznosc.zmiennych, echo=FALSE, cache=TRUE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
kable(drzewo$variable.importance, col.names="Ważność zmiennej", format="latex",
      caption="Ważność zmiennych na podstawie konstrukcji drzewa klasyfikacyjnego")
@

Na podstawie otrzymanych ważności zmiennych
(tab.\ref{tab:waznosc.zmiennych}), spróbujemy skonstruować dwa modele,
ograniczając ilość zmiennych w modelu.

<<drzewa.ograniczone.zmienne.pokaz, eval=FALSE>>=
wyniki["drzewo1"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, rpart
)
wyniki["drzewo2"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, rpart
)
wyniki["drzewo1.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, nasze.drzewo
)
wyniki["drzewo2.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo()
)

wyniki
@

<<drzewa.ograniczone.zmienne, echo=FALSE, cache=TRUE>>=
wyniki["drzewo1"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, rpart
)
wyniki["drzewo2"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, rpart
)
wyniki["drzewo1.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, nasze.drzewo
)
wyniki["drzewo2.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
)
kable(
  wyniki[c("drzewo1", "drzewo1.przyciete", "drzewo2", "drzewo2.przyciete")],
  col.names=c("drzewo 1", "przycięte drzewo 1","drzewo 2", "przycięte drzewo 2"),
  format="latex",
  caption="Wyniki dla drzew z ograniczoną ilością zmiennych niezależnych")
@

Na podstawie wyników z tabeli \ref{tab:drzewa.ograniczone.zmienne},
widzimy, że spośród tych modeli, najlepiej sprawdza się model ograniczony do
6 najważniejszych zmiennych, który jest przycinany po konstrukcji.
Ponadto, możemy zauważyć, że te wyniki są lepsze niż
wstępne wyniki(tab.\ref{przyciete.drzewo}).


TODO:
Aby spróbować poprawić wyniki jeszcze bardziej, spróbujemy skorzystać
z komitetów drzew klasyfikacyjnych. W tym celu wykorzystamy metody
bagging, boosting oraz lasy losowe.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Podsumowanie}
Najważniejsze wnioski, jakie udało się wysnuć na podstawie przeprowadzonych analiz/eksperymentów. Wnioski mogą być wypunktowane, tzn.:



Można by zacytować zbióóóóóóóóór danych
\end{document}
