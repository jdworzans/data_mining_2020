\documentclass[12pt, a4paper]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% dodatkowe pakiety LaTeX'a
\usepackage[OT4]{polski}
\usepackage[utf8]{inputenc}
\usepackage[top=2.5cm, bottom=2.5cm, left=2cm, right=2cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage[colorlinks=true, linkcolor=blue]{hyperref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ustawienia globalne
<<ustawienia_globalne, echo=FALSE, warning=FALSE>>=
library(knitr)
opts_chunk$set(
  fig.path='figure/',
  fig.align='center',
  fig.pos='H',
  fig.width=5,
  fig.height=4)
@


\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% strona tytulowa


\title{Raport nr 3}
\author{Emilia Kowal [249716], Jakub Dworzański [249703]}
\maketitle
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Zad. 1
\section{Klasyfikacja na bazie modelu regresji liniowej}

<<>>=
library(datasets)
library(ggplot2)
library(varhandle)
@


\subsection{Krótki opis zagadnienia}
W tej sekcji będziemy zajmować się klasyfikacją opartą na modelu regresji liniowej, z wykorzystaniem metody najmniejszych kwadratów.
Analizy dokonujemy na zbiorze danych \textit{iris}:

<<echo=FALSE>>=
data(iris)
attach(iris)
kable(head(iris), format="latex", caption="Przykładowe dane")
@

Zbiór danych zawiera 150 wierszy z informacjami na temat 3 gatunków irysów: \textit{setosa}, \textit{versicolor} i \textit{virginica}. Zmienne dostarczają nam wiedzy o szerokości i długości działki kielicha oraz płatka.
Dane nie zawierają obserwacji brakujących.

Po dokonaniu analizy postaramy się odpowiedzieć na pytania o:
\begin{itemize}
\item skuteczność klasyfikacji opartej na modelu regresji liniowej;
\item wpływ ilości klas na jakość predykcji;
\item maskowanie klas;
\item wpływ składników wielomianowych stopnia 2 na dokładność klasyfikacji.
\end{itemize}

\subsection{Opis eksperymentów/analiz}

\begin{itemize}
\item W pierwszym kroku, budując model klasyfikacyjny, dokonujemy podziału zbioru danych na treningowy oraz testowy w stosunku 1:5. 
\item Następnie na podstawie danych uczących kostruujemy klasyfikator i predykujemy etykiety dla zbioru treningowego i testowego.
\item Potem dokonujemy oceny jakości modelu, wyznaczamy macierz pomyłek, przeprowadzamy analizę graficzną klasyfikatora.
\item Ostatecznie badamy wpływ składników wielomianowych stopnia 2 na dokładność modelu regresji liniowej, dodając do zbioru danych cechę Petal.Length$^2$ oraz Petal.Width$*$Sepal.Length.
\end{itemize}

\subsection{Wyniki}

Na podstawie wykresu widzimy, iż poszczególne klasy zwierają tyle samo obiektów:

<<klasy, echo=FALSE, fig.cap="Przynależności do klas.">>=
ggplot(data=iris, aes(x=Species)) + 
  geom_bar() + labs(x="gatunek", y="liczebność gatunku")
@

\subsubsection{Analiza skuteczności modelu dla zbioru danych bez składników wielomianowych stopnia 2.}
Dokonujemy podziału zbioru danych na trengingowe oraz testowe:

<<train_test_split>>=
smp_size <- floor(0.8 * nrow(iris))
set.seed(43)
train_ind <- sample(seq_len(nrow(iris)), size=smp_size)
# pamiętamy, żeby uwzględnić wyraz wolny w modelu regresji
X_train <- cbind(rep(1,120),iris[train_ind, 1:4])
X_test <- cbind(rep(1,30),iris[-train_ind, 1:4])
Y_train <- iris[train_ind, 5]
Y_test <- iris[-train_ind, 5]
@


Konwertujemy zmienną kategoryczną na macierz wskaźnikową z wykorzystaniem funkcji \textit{to.dummy} z pakietu \textit{varhandle}:
<<konwersja>>=
Y_train_bin <- to.dummy(Y_train, prefix="species")
Y_test_bin <- to.dummy(Y_test, prefix="species")
@

Konstrujujemy klasyfikator regresji liniowej metodą najmniejszych kwadratów:

<<classifier>>=
B <- solve(t(X_train)%*%as.matrix(X_train)) %*% t(X_train) %*% as.matrix(Y_train_bin)
@
Dla zbioru testowego otrzymujemy następujące wyniki: \ref{tab:result_test}.

<<result_test, echo=FALSE>>=
Y_result <- as.matrix(X_test) %*% B
klasy <- levels(Species)
  maks.ind <- apply(Y_result, 1, FUN=function(x) which.max(x))
  prognozowane.etykietki <- klasy[maks.ind]
  rzeczywiste.etykietki <- Y_test
  
  conf_mtrx <- table(rzeczywiste.etykietki, prognozowane.etykietki)
  rownames(conf_mtrx) <- c("setosa.true", "versicolor.true", "virginica.true")
  kable(conf_mtrx,caption="Skuteczność estymatora dla danych testowych.", 
        format="latex")
  #sum(diag(conf_mtrx))/length(Y_test)
@
Dostajemy dokładność na poziomie: 73\%. \\
<<result_test_plot, echo=FALSE, fig.cap="Wizualizacja wyników dla zbioru testowego.", fig.height=6, fig.width=8>>=
matplot(Y_result, ylab="Predykcja")
  abline(v=c(10,20), lty=2, col="grey")
  legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@
Na wykresie \ref{fig:result_test_plot} widzimy, że dominują klasy setosa i virginica, zatem wnioskujemy, iż versicolor jest zamaskowana (zdominowana) przez pozostałe dwie klasy. \\
Następnie dokonujemy analizy zgodności dla zbioru treningowego w celu uniknięcia przetrenowania modelu. Otrzymujemy następujące rezultaty: \ref{tab:result_train}.

<<result_train, echo=FALSE>>=
Y_result.train <- as.matrix(X_train) %*% B

maks.ind.train <- apply(Y_result.train, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.train <- klasy[maks.ind.train]
rzeczywiste.etykietki.train <- Y_train

conf_mtrx.train <- table(rzeczywiste.etykietki.train, prognozowane.etykietki.train)
rownames(conf_mtrx.train) <- c("setosa.true", "versicolor.true", "virginica.true")

kable(conf_mtrx.train, caption="Skuteczność estymatora dla danych treningowych.", 
      format="latex")
#sum(diag(conf_mtrx.train))/length(Y_train)
@
Dostajemy zgodność na poziomie: 88\%, co mogłoby wskazywać na to, iż model jest przetrenowany. Aby uniknąć takiej sytuacji, w dalszej analizie należałoby przeprowadzić np. cross validation test, który dałby nam rzetelniejsze wyniki dla całego zbioru danych. Również fakt, iż w zbiorze nie mamy wielu rekordów, działa niekorzystnie na skuteczność modelu regresji. \\
Wyniki przedstawione graficznie:
<<result_train_plot, echo=FALSE, fig.cap="Wizualizacja wyników dla zbioru treningowego.", fig.height=6, fig.width=8>>=
matplot(Y_result.train, ylab="Predykcja")
  abline(v=c(40,80), lty=2, col="grey")
  legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@

\subsubsection{Analiza skuteczności modelu dla zbioru danych ze składnikami wielomianowymi stopnia 2.}
Do konstrukcji drugiego modelu regresji wykorzystujemy zbiór z wyjściowymi cechami oraz zmiennymi wielomianowymi stopnia 2. 
<<new_dataset, echo=FALSE>>=
  new_iris <- cbind(Petal.Length^2, Petal.Width^2, 
                  Sepal.Length^2, Sepal.Width^2,
                  Petal.Length*Petal.Width,
                  Petal.Width*Sepal.Length,
                  Petal.Length*Sepal.Width,
                  Petal.Length*Sepal.Length,
                  Petal.Width*Sepal.Width,
                  Sepal.Length*Sepal.Width,
                  iris)

X_train_n <- cbind(rep(1,120),new_iris[train_ind, 1:14])
X_test_n <- cbind(rep(1,30),new_iris[-train_ind, 1:14])
Y_train_n <- new_iris[train_ind, 15]
Y_test_n <- new_iris[-train_ind, 15]

# konwersja zmiennej kategorycznej na macierz wskaźnikową:
Y_train_bin_n <- to.dummy(Y_train_n, prefix="species")
Y_test_bin_n <- to.dummy(Y_test_n, prefix="species")

# konstrukcja klasyfikatora regresji liniowej metodą najmniejszych kwadratów:
B1 <- solve(t(X_train_n)%*%as.matrix(X_train_n)) %*% t(X_train_n) %*% as.matrix(Y_train_bin_n)

# wartości progonozowane:
Y_result_n <- as.matrix(X_test_n) %*% B1
@

Dla estymatora, skonstruowanego w oparciu o nowy zbiór danych, dla zbioru testowego otzymujemy następujące wyniki: \ref{tab:result_test_new}.

<<result_test_new, echo=FALSE>>=
maks.ind.n <- apply(Y_result_n, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.n <- klasy[maks.ind.n]
rzeczywiste.etykietki.n <- Y_test_n

conf_mtrx_n <- table(rzeczywiste.etykietki.n, prognozowane.etykietki.n)
rownames(conf_mtrx_n) <- c("setosa.true", "versicolor.true", "virginica.true")
kable(conf_mtrx_n, caption="Skuteczność nowego estymatora dla zbioru testowego.",format="latex")
@

Dostajemy zgodność na poziomie: 97\%, zatem otrzymujemy znacznie skuteczniejszy estymator dla zbioru danych ze składnikami wielomianowymi stopnia 2. 

<<result_test_new_plot, echo=FALSE, fig.cap="Wizualizacja wyników nowego estymatora dla zbioru testowego.", fig.height=6, fig.width=8>>=
matplot(Y_result_n, ylab="Predykcja")
abline(v=c(10,20), lty=2, col="grey")
legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@
Na podstawie wykresu \ref{fig:result_test_new_plot} zauważamy, że dla nowego zbioru danych, klasy setosa oraz virginica nie dominują w sposób znaczny klasy versicolor. Wnioskujemy, że z tego też względu otrzymujemy znacznie lepszy rezultat dla modelu regresji. \\
Zbadamy jeszcze skuteczność estymatora dla zbioru treningowego: \ref{tab:result_train_new}.

<<result_train_new, echo=FALSE>>=
Y_result.train.n <- as.matrix(X_train_n) %*% B1

maks.ind.train.n <- apply(Y_result.train.n, 1, FUN=function(x) which.max(x))
prognozowane.etykietki.train.n <- klasy[maks.ind.train.n]
rzeczywiste.etykietki.train.n <- Y_train_n

conf_mtrx.train.n <- table(rzeczywiste.etykietki.train.n, prognozowane.etykietki.train.n)
rownames(conf_mtrx.train.n) <- c("setosa.true", "versicolor.true", "virginica.true")
kable(conf_mtrx.train.n, caption="Skuteczność nowego estymatora na zbiorze treningowym.", 
      format="latex")
@

Otrzymujemy zgodność na poziomie 98\%.

<<resutl_train_new_plot, echo=FALSE, fig.height=6, fig.width=8>>=
matplot(Y_result_n, ylab="Predykcja")
abline(v=c(40,80), lty=2, col="grey")
legend(x="top", legend=paste(1:3,levels(Species)), col=1:3, text.col=1:3)
@


\subsection{Podsumowanie}
\begin{itemize}
\item Po przeprowadzeniu analiz wnioskujemy, iż model oparty na regresji liniowej jest skuteczny, kiedy zależności między zmiennymi są liniowe. Jak wiadomo, zazwyczaj jednak mamy styczność ze zmiennymi o zależnościach nieliniowych, gdzie estymator regresji może dawać niesatysfakcjonujące rezultaty. Widzimy, iż po dodaniu do zbioru danych cech wielomianowych stopnia 2, skuteczność klasyfikatora znacznie wzrasta
\item Ponadto, kiedy tworzymy model klasyfikacyjny dla większej liczby klas, regresja liniowa jest narażona na problem maskowania klas, który również wpływa niekorzystnie na wyniki estymatora.
\item Tworząc model regresji należy pamiętać o zapobiaganiu jego przetrenowania wykorzystując metody takie, jak \textit{cross-validation}  lub \textit{bagging}.
\end{itemize}
Podsumowując regresja liniowa zakłada, iż w zbiorze zmienne mają liniowe zależności. Pracując nad bardziej złożonym problemem klasyfikacyjnym, dla dużej liczby klas, wybór odpowiednich składników wielomianowych może być czasochłonne, a przez to nieefektywne. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Zad. 2
<<ustalenie.ziarna, echo=FALSE>>=
set.seed(14052020)
@

<<biblioteki, echo=FALSE, eval=TRUE, warning=FALSE, results=FALSE, message=FALSE>>=
library(mlbench)
library(ipred)
library(rpart)
library(e1071)
library(purrr)
library(naivebayes)
library(randomForest)
@

\section{Porównanie metod klasyfikacji}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Krótki opis zagadnienia}

W tym ćwiczeniu, mamy do czynienia ze zbiorem danych o odłamkach szkła.
<<przykladowe_dane, echo=FALSE>>=
data(Glass)
kable(head(Glass), caption="Przykładowe dane", format="latex")
@
<<przykladowe_dane_pokaz ,eval=FALSE>>=
data(Glass)
head(Glass)
@

% https://archive.ics.uci.edu/ml/datasets/glass+identification

Dane składają się z informacji, dotyczących 214 odłamków.
Każdy z rekordów składa się ze współczynnika załamania światła ({\verb+RI+})
oraz stężenia różnych pierwiastków (w odpowiadających tlenkach)
w badanych próbkach.
Oprócz tego, posiadamy informacje o typie szkła,
z którego odłamkiem mamy do czynienia.
Razem, mamy 7 takich kas, z których tylko 6 występuje w danych.
Łącznie, dla każdej próbki występuje 10 cech.
Ponadto, w zbiorze nie ma wartości brakujących.

Naszym celem będzie konstrukcja klasyfikatora, który będzie
przyporządkowywał odłamki szkła do odpowiednich klas.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Opis eksperymentów/analiz}

Skorzystamy z najczęściej stosowanych, podstawowych metod.
Będą to metoda k-najbliższych sąsiadów (k-NN, \emph{k-Nearest Neighbours}),
naiwny klasyfikator bayesowski (NBC, \emph{Naive Bayesian Classifier}) oraz
drzewo klasyfikacyjne (\emph{classification tree}).

Ponadto, postaramy się dobrać odpowiednie parametry dla tych modeli.
Spróbujemy również dokonać pewnych przekształceń danych, czy też
wybrać pewne podzbiory cech, aby sprawdzić, czy uda nam się w ten
sposób poprawić jakość klasyfikatora.

Po zbadaniu podstawowych metod, spróbujemy również porównać
je z metodami bardziej zaawansowanymi i wyrafinowanymi.

Aby ocenić jakość predykcji, będziemy korzystać
z 5-, 10-krotnego sprawdzianu krzyżowego
oraz sprawdzianu krzyżowego typu leave-one-out
(\emph{5-, 10-, leave-one-out Cross Validation}).
Ponadto, wykorzystamy metodę \emph{bootstrap} oraz \emph{632+}.

Zastosujemy do tego następujące funkcje pomocnicze,
odpowiadające za uczenie modelu oraz estymację błędów.

<<funkcje.pomocnicze>>=
mypredict <- function(model, newdata) predict(model, newdata, type="class")
nasz.errorest <- function(formula, data, model, ...){
  cv.10 <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = 10), ...
  )$error
  cv.5 <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = 5), ...
  )$error
  leave.one.out <- errorest(
    formula=formula, data=data, model=model, predict=mypredict,
    estimator="cv", est.para=control.errorest(k = nrow(data)), ...
  )$error
  bootstrap <- errorest(
    formula=formula, data=data, model=model,
    predict=mypredict, estimator="boot", ...
  )$error
  err632plus <- errorest(
    formula=formula, data=data, model=model,
    predict=mypredict, estimator="632plus", ...
  )$error
  return(data.frame(
    paste0(
      round(100*c(cv.5, cv.10, leave.one.out, bootstrap, err632plus), 2),
      "%"
    ),
    row.names=c(
      "CV5", "CV10", "CV leave-one-out",
      "bootstrap (25 powtórzeń)",
      "632+ (25 powtórzeń)"
    )
  ))
}
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Wyniki}

\subsubsection{Modele z domyślnymi wartościami dla wszystkich cech}
Nasze badania rozpoczniemy od estymacji błędu klasyfikacji dla modeli
z domyślnymi parametrami i bez wcześniejszych przekształceń danych.
Pozwoli to nam później ocenić ewentualną poprawę wyników.

<<surowe.porowananie.pokaz, eval=FALSE>>=
wyniki <- nasz.errorest(Type ~ ., Glass, ipredknn)
wyniki["drzewo"] <- nasz.errorest(Type ~ ., Glass, rpart)
wyniki["nb"] <- nasz.errorest(Type ~ ., Glass, naiveBayes)

wyniki
@

<<surowe.porownanie, echo=FALSE, cache=TRUE>>=
wyniki <- nasz.errorest(Type ~ ., Glass, ipredknn)
names(wyniki) <- c("k.nn")
wyniki["drzewo"] <- nasz.errorest(Type ~ ., Glass, rpart)
wyniki["nb"] <- nasz.errorest(Type ~ ., Glass, naiveBayes)

kable(
  wyniki,
  format="latex",
  col.names=c("k-NN", "drzewo klasyfikacyjne", "naiwny klasyfikator bayesowski"),
  caption="Porównanie błędu klasyfikacji dla modeli z domyślnymi parametrami.",
  digits=2,
  align="r"
)
@

W tabeli \ref{tab:surowe.porownanie} widzimy, że dla domyślnych parametrów,
z całymi danymi najlepiej radzą sobie metoda k-najbliższych sąsiadów oraz
drzewa klasyfikacyjne. Można również zauważyć, że naiwny klasyifkator bayesowski
zadziałał w tym przypadku zdecydowanie gorzej.

Postaramy się znaleźć odpowiednie podzbiory cech oraz parametry dla poszczególnych
modeli, aby poprawić otrzymane rezultaty.

\subsubsection{Analiza opisowa}


\subsubsection{Metoda k-najbliższych sąsiadów}
Sprawdzimy, jak możemy zmienić jakość modelu k-NN, zmieniając
wartość \emph{k}, która reprezentuje liczbę sąsiadów, na podstawie których
wybieramy klasę.

<<knn.k.pokaz, eval=FALSE>>=
knn <- function(k) partial(ipredknn, k=k)
wyniki["k.nn.1"] <- nasz.errorest(Type ~ ., Glass, knn(1))
wyniki["k.nn.2"] <- nasz.errorest(Type ~ ., Glass, knn(2))
wyniki["k.nn.3"] <- nasz.errorest(Type ~ ., Glass, knn(3))
wyniki["k.nn.4"] <- nasz.errorest(Type ~ ., Glass, knn(4))
wyniki["k.nn.10"] <- nasz.errorest(Type ~ ., Glass, knn(10))
wyniki["k.nn.15"] <- nasz.errorest(Type ~ ., Glass, knn(15))

wyniki
@

<<knn.k, echo=FALSE, cache=TRUE>>=
knn <- function(k) partial(ipredknn, k=k)
wyniki["k.nn.1"] <- nasz.errorest(Type ~ ., Glass, knn(1))
wyniki["k.nn.2"] <- nasz.errorest(Type ~ ., Glass, knn(2))
wyniki["k.nn.3"] <- nasz.errorest(Type ~ ., Glass, knn(3))
wyniki["k.nn.4"] <- nasz.errorest(Type ~ ., Glass, knn(4))
wyniki["k.nn.10"] <- nasz.errorest(Type ~ ., Glass, knn(10))
wyniki["k.nn.15"] <- nasz.errorest(Type ~ ., Glass, knn(15))

kable(
  wyniki[c("k.nn.1", "k.nn.2", "k.nn.3", "k.nn.4", "k.nn",
           "k.nn.10", "k.nn.15")],
  col.names=c("k=1", "k=2", "k=3", "k=4", "k=5", "k=10", "k=15"),
  format="latex",
  caption="Sprawdzenie poprawy błędu klasyfikacji po zmianie parametru k."
)
@

Na podstawie tabeli \ref{tab:knn.k} widzimy, że istotnie możemy znacznie
poprawić wyniki, zmieniając wyłącznie parametr k. Ponadto, widzimy, że
w porównaniu do pozostałych wartości, model bardzo dobrze działa dla k=1.

Aby poprawić wyniki, możemy jeszcze spróbować ograniczyć liczbę zmiennych,
stosując PCA.

<<knn.k.pca.pokaz, eval=FALSE>>=
Glass.pca <- prcomp(subset(Glass, select=-c(Type)), retx=T, center=T, scale.=T)
Glass.pca <- data.frame(Glass.pca$x[,1:4])
Glass.pca$Type <- Glass$Type

wyniki["k.nn.1.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(1))
wyniki["k.nn.2.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(2))
wyniki["k.nn.3.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(3))
wyniki["k.nn.4.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(4))
wyniki["k.nn.5.pca"] <- nasz.errorest(Type ~ ., Glass.pca, ipredknn)
wyniki["k.nn.10.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(10))
wyniki["k.nn.15.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(15))

wyniki
@

<<knn.k.pca, echo=FALSE, cache=TRUE>>=
Glass.pca <- prcomp(subset(Glass, select=-c(Type)), retx=T, center=T, scale.=T)
Glass.pca <- data.frame(Glass.pca$x[,1:4])
Glass.pca$Type <- Glass$Type

wyniki["k.nn.1.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(1))
wyniki["k.nn.2.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(2))
wyniki["k.nn.3.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(3))
wyniki["k.nn.4.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(4))
wyniki["k.nn.5.pca"] <- nasz.errorest(Type ~ ., Glass.pca, ipredknn)
wyniki["k.nn.10.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(10))
wyniki["k.nn.15.pca"] <- nasz.errorest(Type ~ ., Glass.pca, knn(15))

kable(
  wyniki[c("k.nn.1.pca", "k.nn.2.pca", "k.nn.3.pca", "k.nn.4.pca", "k.nn.5.pca",
           "k.nn.10.pca", "k.nn.15.pca")],
  col.names=c("k=1", "k=2", "k=3", "k=4", "k=5", "k=10", "k=15"),
  format="latex",
  caption="Sprawdzenie poprawy błędu klasyfikacji po użyciu pierwszych 4 głównych składowych"
)
@

W tabeli \ref{tab:knn.k.pca} widzimy, że po redukcji danych do 4 pierwszych
głównych składowych (wyjaśniają one około 80\% wariancji), wyniki są porównywalne
z tymi bez redukcji danych.

Widzimy zatem, że najlepsze wyniki dla metody k-NN otrzymaliśmy dla k=1.


\subsubsection{Drzewo klasyfikacyjne}
Skorzystamy z metody jednego odchylenia standardowego,
przedstawionej na wykładzie, aby zmniejszać złożoność
drzewa po uczeniu.
<<przyciete.drzewo.pokaz, eval=FALSE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["przyciete.drzewo"] <- nasz.errorest(Type ~ ., Glass, nasze.drzewo)
wyniki[c("drzewo", "przyciete.drzewo")]
@

<<przyciete.drzewo, echo=FALSE, cache=TRUE>>=
nasze.drzewo <- function(...){
  drzewo <- rpart(..., minsplit=1)
  cptable <- data.frame(drzewo$cptable)
  xerror.min <- min(cptable$xerror)
  xstd.min <- max(cptable[which(cptable$xerror == xerror.min),]$xstd)
  ponizej.1.sd <- cptable[which(cptable$xerror < xerror.min + xstd.min),]$CP
  return(prune(drzewo, cp=max(ponizej.1.sd)))
}
wyniki["przyciete.drzewo"] <- nasz.errorest(Type ~ ., Glass, nasze.drzewo)
kable(
  wyniki[c("drzewo", "przyciete.drzewo")],
  col.names=c("drzewo", "przycięte drzewo"),
  format="latex",
  caption="Sprawdzenie poprawy błędu klasyfikacji po przycięciu drzewa"
)
@


W tabeli \ref{tab:przyciete.drzewo} widzimy, że przycięcie drzewa za pomocą
metody 1 odchylenia standardowego pozwala nie zmienia znacząco wyników.
Możemy jednak przypuszczać, że przycięte drzewo ma lepsze właściwości
uogólniające.

Możemy również spróbować wybrać najważniejsze zmienne, tworząc drzewo klasyfikacyjne,
a następnie skonstruować model z wykorzystaniem tych cech.

<<waznosc.zmiennych.pokaz, eval=FALSE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
drzewo$variable.importance
@
<<waznosc.zmiennych, echo=FALSE, cache=TRUE>>=
drzewo <- rpart(Type~., Glass, minsplit=1)
kable(drzewo$variable.importance, col.names="Ważność zmiennej", format="latex",
      caption="Ważność zmiennych na podstawie konstrukcji drzewa klasyfikacyjnego")
@

Na podstawie otrzymanych ważności zmiennych
(tab.\ref{tab:waznosc.zmiennych}), spróbujemy skonstruować dwa modele,
ograniczając ilość zmiennych w modelu.

<<drzewa.ograniczone.zmienne.pokaz, eval=FALSE>>=
wyniki["drzewo1"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, rpart
)
wyniki["drzewo2"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, rpart
)
wyniki["drzewo1.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, nasze.drzewo
)
wyniki["drzewo2.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo()
)

wyniki
@

<<drzewa.ograniczone.zmienne, echo=FALSE, cache=TRUE>>=
wyniki["drzewo1"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, rpart
)
wyniki["drzewo2"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, rpart
)
wyniki["drzewo1.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca, Glass, nasze.drzewo
)
wyniki["drzewo2.przyciete"] <- nasz.errorest(
  Type ~ RI + Al + Ca + Mg + Na + Ba, Glass, nasze.drzewo
)
kable(
  wyniki[c("drzewo1", "drzewo1.przyciete", "drzewo2", "drzewo2.przyciete")],
  col.names=c("drzewo 1", "przycięte drzewo 1","drzewo 2", "przycięte drzewo 2"),
  format="latex",
  caption="Wyniki dla drzew z ograniczoną ilością zmiennych niezależnych")
@

Na podstawie wyników z tabeli \ref{tab:drzewa.ograniczone.zmienne},
widzimy, że spośród tych modeli, najlepiej sprawdza się model ograniczony do
6 najważniejszych zmiennych, który jest przycinany po konstrukcji.
Ponadto, możemy zauważyć, że te wyniki są lepsze niż
wstępne wyniki(tab.\ref{tab:przyciete.drzewo}).

\subsubsection{Naiwny klasyfikator bayesowski}
W tej części postaramy się poprawić wyniki naiwnego klasyfikatora bayesowskiego.

Wszystkie zmienne w naszym zbiorze to zmienne ciągłe. Domyślnie, implementacja
naiwnego klasyfikatora bayesowskiego dokonuje założenia o rozkładzie normalnym
zmiennych ciągłych. Dlatego też rozpoczniemy od zmiany tego założenia na
estymację gęstości rozkładów zmiennych ciągłych.

<<nb.kernel.pokaz, eval=FALSE>>=
wyniki["nb.kernel"] <- nasz.errorest(Type~., Glass, naive_bayes, usekernel=T)
wyniki[c("nb", "nb.kernel")]
@
<<nb.kernel, echo=FALSE, cache=TRUE, warning=FALSE>>=
wyniki["nb.kernel"] <- nasz.errorest(Type~., Glass, naive_bayes, usekernel=T)

kable(
  wyniki[c("nb", "nb.kernel")],
  col.names=c("rozkład normalny", "estymacja gęstości rozkładu"),
  caption="Porównanie wyników naiwnego klasyfikatora bayesowskiego z użyciem estymacji gęstości rozkładu"
)
@

W tabeli \ref{tab:nb.kernel} widzimy, że prosta zmiana pozwoliła na
znaczną poprawę wyników.

\subsubsection{Komitety klasyfikatorów}
Po zbadaniu i próbie poprawy klasyfikatorów k-NN, drzew klasyfikacyjnych
oraz naiwnego klasyfikatora bayesowskiego, spróbujemy wykorzystać komitety klasyfikatorów,
aby sprawdzić, czy otrzymamy w ten sposób porównywalne wyniki.

Wykorzystamy w tym celu las losowy.

<<random.forest.pokaz, eval=FALSE>>=
wyniki["random.forest"] <- nasz.errorest(Type~., Glass, randomForest)
wyniki
@

<<random.forest, echo=FALSE, cache=TRUE>>=
wyniki["random.forest"] <- nasz.errorest(Type~., Glass, randomForest)
kable(
  wyniki[c("nb.kernel", "k.nn.1", "drzewo2.przyciete", "random.forest")],
  col.names=c("NBC", "k-NN", "drzewo klasyfikacyjne", "las losowy"),
  format="latex",
  caption="Porównanie najlepszych wyników"
)
@

W tabeli \ref{tab:random.forest} widzimy, że, nawet dla domyślnych parametrów, las losowy
poradził sobie z danymi najlepiej spośród przygotowanych przez nas klasyfikatorów.
Stąd, możemy przypuszczać, że poprawa parametrów, mogłaby jeszcze bardziej zwiększyć
różnicę między tymi klasyfikatorami.


\subsection{Podsumowanie}

Udało nam się skonstruować działające klasyfikatory.
Warto zaznaczyć, że klasyfikator, który wskazywałby losową klasę,
miałby błąd predykcji na poziomie 83\%, natomiast taki,
który wskazywałby zawsze klasę 2 (najliczniejszą), mógłby osiągnąć
błąd na poziomie 64\%.

Nam, natomiast, udało się osiągnąć błąd predykcji rzędu 20\%, co
wydaje się być dość dobrym wynikiem.

Ponadto, dla każdego z podstawowych klasyfikatorów, udało nam się
poprawić jego jakość. Wykorzystaliśmy do tego zmianę parametrów,
ale też redukcję wymiaru, czy też ilości cech. Ponadto korzystaliśmy
z metod, które zapobiegają przeuczeniu modelu, takich jak przycinanie
drzewa.

Spośród najprostszych klasyfikatorów, najlepsze rezultaty otrzymaliśmy
dla metody k-najbliższych sąsiadów. Wyniki dla drzewa klasyfikacyjnego były
nieznacznie gorsze. Nie udało nam się jednak zbliżyć do tych wyników dla
naiwnego klasyfikatora bayesowskiego. Możemy przypuszczać, że jest to związane
z tym, że mamy do czynienia przede wszystkim z cechami ciągłymi, których rozkłady
nie są znane.

Na zakończenie, należy zapamiętać, że korzystając z nowszych, bardziej
zaawansowanych i wyrafinowanych algorytmów, takich jak las losowy, byliśmy
w stanie osiągnąć rewelacyjne rezultaty, które są zdecydowanie lepsze
niż wyniki dla podstawowych klasyfikatorów.


\end{document}
